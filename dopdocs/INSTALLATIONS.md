# Инструкция по применению модуля развёртывания и настройки ceph

[[_TOC_]]

Здесь приведена инструкция которой необходимо следовать для развёртывания кластера ceph

Предварительные шаги
--------

Модуль развёртывания и настройки ceph (далее - модуль ceph) предназначен для применения в вычислительной среде, в которой ранее не применялся модуль ceph. То есть:

* На всех серверах кластера, ceph не был установлен. Отсутствует LVM на дисках сервера, на которые будут устанавливаться OSD.
* В PuppetDB отсутствуют сертификаты для серверов кластера ceph

Если модуль ceph применяется в вычислительной среде, где ранее развёртывался ceph, то результат применения модуля трудно прогнозируемый. В этом случае необходимо, как минимум, удалить сертификаты серверов кластера ceph, удалить ранее созданные ceph LVM (LVM созданный ceph) c дисков сервера, на которые ceph будет устанавливаться.

Для удаления сертификатов серверов кластера ceph (если они сохранились от предыдущего развёртывания), необходимо на Cервере управления (сервере puppet) в командной строке выполнить:

```{.bash}
sudo puppet cert --clean "fqdn"
```

где "fqdn" - FQDN сервера ceph (основного интерфейса сервера, не интерфейса передачи данных, без перфикса "ib"), например, cephmon06.ppoi.ensk. Вместо "fqdn" одного сервера может быть указано регулярное выражение описывающее "fqdn" нескольких серверов, например, pnode{101..116}.ppoi.ensk.

Для удаления ранее созданных ceph LVM (LVM созданный ceph) c дисков сервера на всех серверах на которых будут развертываться OSD, в командной строке выполнить:

Убедиться, что lvm присутствуют на дисках:
```{.bash}
sudo /sbin/pvs | grep ceph
```

Если присутствуют, то нужно удалить lvm. На каждом сервере выполнить:

```{.bash}
for hdd in /dev/sd{b..g}; do /sbin/vgremove -f pvs -ovg_name --noheadings ${hdd}; done
for hdd in /dev/sd{b..g}; do if [ -b ${hdd} ]; then /sbin/pvremove ${hdd}; fi; done
```
Где вместо `/dev/sd{b..g}` указать устройства, на которых нужно удалить lvm от прошлой установки ceph.  
(!!! Не работает пока. Лучше так не делать. Так как обязательно забудем установить параметр `$forceDelLvmOsdHdd` в `false`, сразу после развертывания и при следующем применении модулей puppet удалим только что созданый нами LVM). Вместо ручного удаления lvm, можно установить параметр `$forceDelLvmOsdHdd` в `true` в манифесте init.pp модуля ceph. Это удалит ceph LVM c дисков, которые мы указали в параметре $nameDisksOsdHdd.
!!!Важно. После того, как модуль ceph применится (удалятся ceph LVM созданные в предыдущей "старой" установке ceph) необходимо сразу же установить параметр `$forceDelLvmOsdHdd` в `false` в манифесте init.pp модуля ceph.

Перед развёртыванием кластера ceph необходимо произвести следующие проверки и настройки:

* Убедиться, что репозитории настроены и сделано обновление ОС на всех серверах кластера ceph (применён модуль os_base_config):

```{.bash}
apt-get update
apt-get -y dist-upgrade
```
После обновления сервера необходимо его перезагрузить. Это должно делаться сразу после развёртывания ОС. Отсутствие обновления может влиять на работу всего СПО и ОПО.

* Применить модуль отвечающий за настройку интерфейса для передачи данных (обычно Infiniband), чтобы на сервере появился этот интерфейс и ему присвоился IP.

* В DNS внести записи для интерфейса передачи данных (обычно Infiniband) всех серверов ceph. Имя интерфейса передачи данных серверов ceph должно быть равно имени его Ethernet интерфейса с добавлением суффикса ib в конце. Например, если имя Ethernet интерфейса - pnode01, тогда имя Infiniband интерфейса - pnode01ib.

* Убедиться, что время на всех серверах ceph одинаковое, с точностью до миллисекунд (настроен ntpd).

СПО КВР
-------
Перед загрузкой модуля ceph в СПО КВР нужно убедиться, что в файлах *.pp, манифестах, заданы следующие значения по умолчанию:

 Манифест   | Параметр манифеста   | Значение параметра по умолчанию
----------- | -------------------- | ------------------
system.pp   |   $ensure            |  running
mon.pp      |   $ensure            |  undef
mds.pp      |   $ensure            |  undef
osdhdd.pp   |   $ensure            |  running
osdram.pp   |   $ensure            |  running
cli.pp      |   $ensure            |  running
pool.pp     |   $ensure            |  undef
mount.pp    |   $ensure_hdd        |  undef
mount.pp    |   $ensure_ram        |  undef
init.pp     |   $apply_ceph        |  true
init.pp     |   $monitoring_enable |  true
init.pp     |   $first_mon         |  false
init.pp     |   $forceDelLvmOsdRam |  false
init.pp     |   $forceDelLvmOsdHdd |  false

Порядок следования модулей в КСУ СПО КВР имеет важное значение. Известно, что модуль ceph, должен следовать ниже по списку (применяться после) следующих модулей:

* os_base_config
* Infiniband
* ntp

Следуя инструкциям на СПО КВР:

* Добавить модуль ceph в СПО КВР
* Добавить модуль ceph в соответствующее КСУ
* В КСУ в модуль ceph добавить следующие ресурсы. Последовательность добавления ресурсов строгая, такая, какая приведена здесь:

```
ceph
ceph::system
ceph::mon
ceph::mds
ceph::osdhdd
ceph::osdram
ceph::cli
ceph::pool:hdd
ceph::pool:ram
ceph::mount
ceph::shareresources
```

Перед развёртыванием
--------------------
Прежде чем приступать к развёртыванию кластера ceph необходимо спроектировать и обосновать его конфигурацию. Под конфигурацией полагается количество и расположение на серверах сервисов ceph - MON, MDS, типы и количество OSD, а также настройки pools.
Конфигурация ceph зависит, в том числе, от следующих факторов:
* Сценария использования кластера ceph: какие данные будут в кластере и как они будут использоваться
* Требуемой отказоустойчивости
* Желаемой скорости доступа к данным
* Желаемой доступности данных

Исходя из нашего опыта проектирования и использования кластера ceph, мы можем сформулировать пример следующей конфигурации кластера:
* Необходимо равномерно распределять компоненты кластера ceph по доменам отказа
* Уровень отказоустойчивости не должен быть менее трёх
* В кластере необходимо развернуть не менее 3 и не более 5-ти серверов MON. Важно, количество серверов MON должно быть нечетным. Количество серверов MON более 5(7, 9...) требует обоснования.
* В кластере необходимо развернуть не менее 3 серверов MDS. Ограничения сверху количества развёртываемых MDS зависит от параметра `$max_mds_fss`, количества pool ceph и требуемой отказоустойчивости. Нужны обоснованные причины, чтобы увеличить `$max_mds_fss` относительно его значения по умолчанию равного 1. Изменять параметр `$max_mds_fss` со значения по умолчанию не рекомендуется.
* Сервисы MON и MDS не устанавливаются на одном и том же сервере\*
* Сервисы OSD располагаются на всех дисках всего кластера по одному osd на один диск.
* OSD на RAM дисках не используются.\*
* Сервис CLI располагается на всех серверах кластера ceph.

\* Требования рекомендуемые, но мы их нарушаем.

Подготовка к развёртыванию ceph
-------------------------------

* В web интерфейсе СПО КВР, в КСУ, задать значения следующим параметрам в манифесте init.pp (отображается как ресурс ceph). Задание параметров в КСУ приведёт к тому, что значения параметров КСУ будут применяться для всех серверов использующих это КСУ. Пояснения к каждому параметру описаны в том манифесте, в котором этот параметр непосредственно используется.

| Параметр          | Комментарий
------------------- | --------------------
$fsid               | Можно сгенерить командой `uuidgen`, находящейся в пакете `uuid-runtime`
$first_mon_ip_data  |
$all_mon_name_data  | Заполнить параметр `all_mon_name_data` именем интерфейса передачи данных (Infiniband) первого MONs. Имя необходимо заключить в кадратные скобки и внутри скобок имя заключить в одинарные кавычки, например: ['cephmon07ib'].
$public_network_data|
$nameDisksOsdHdd    |
$all_mon_ip_data    |
$osd_ram_size, $osdramWeight      | Для `$osd_ram_size` можно просто указать требуемый объём диска в килобайтах. Для `$osdramWeight` сложнее, сначала нужно создать вручную диск требуемого объёма (как создавать OSD можно читать в документации по ceph), а потом командой *ceph osd tree* посмотреть в колонке `WEIGHT` какой `Weight` ceph присвоил этому диску. Увиденное значение уже можно присвоить `$osdramWeight`. Созданный OSD после этого нужно удалить (как удалить OSD можно читать в документации по ceph).
$k, $m              | После создания pools поменять параметры `$k` и `$m` нельзя. https://docs.ceph.com/docs/master/rados/operations/erasure-code/ Параметры `$k`, `$m` отвечают за доступность данных, применяются для erasure coded (EC) pool. `$k` показывает на сколько частей разделяется объект для сохранения каждой части объекта, а `$m` показывает, сколько дополнительных chunks этого объекта создаётся, чтоб иметь возможность восстановить весь объект при выходе из строя `$m` аппаратных средств. На вопрос какие это аппаратные средства отвечает ещё один важный параметр для EC pool это `crush-failure-domain`. У нас `crush-failure-domain`="host", из чего следует, что никакие две части объекта не хранятся на одном сервере. Не стоит задавать большие значения для `$k` и `$m`. `$k`="5", а `$m`="2" являются приемлемыми значениями как для ЦАГИ так и для 1622. EC pools у нас применяются для хранения данных. Для метаданных применяются pools replicated с настройками по умолчанию, то есть с тройной репликацией, что значит каждый объект в pool имеет ещё две копии. И, также, задано, что никакие две копии объекта не хранятся на одном сервере.
$dfwarning          |
$dfcritical         |
$osddfwarning       |
$osddfcritical      |

Развёртывание ceph
----------

Последовательность применения манифестов на кластере для развёртывания ceph имеет строгий порядок следования. Инструкции приведённые здесь обязательно выполняются последовательно одна за другой. Нельзя переходить к следующей инструкции не выполнив предыдущую и не убедившись в том, что ожидаемый результат операции достигнут.

1. Необходимо развернуть первый MON. Для развёртывания необходимо, на сетевом узле, который будет первым MON:
   * В ресурсе *ceph* установить параметр `$first_mon` в `true`
   * В ресурсе `ceph::mon` установить параметр `$ensure` в `running`
   * Если предполагается на этом же сервере установить MDS (что не рекомендуется\*), тогда в ресурсе `ceph::mds` установить параметр `$ensure` в `running`
   * Применить модуль ceph к одному серверу. Нельзя первый и последующие MON-s развёртывать одновременно
при этом, по умолчанию, на этот сервер установятся OSD (на HDD и RAM), а также CLI.

1. Для развёртывания второго, третьего и последующих MON необходимо на сетевых узлах, которые будут вторым, третьим и т.д. MON. Все MON на всех оставшихся серверах можно развёртывать одновременно.
   * В ресурсе `ceph::mon` установить параметр `$ensure` в `running`
   * Если предполагается на этом же сервере установить MDS (что не рекомендуется\*), тогда в ресурсе `ceph::mds` установить параметр `$ensure` в `running`
   * Применить модуль ceph одновременно на втором, третьем и последующих MON.
при этом, по умолчанию, на этот сервер установятся OSD (на HDD и RAM), а также CLI.

1. Для развёртывания первого, второго, третьего и последующих MDS необходимо на сетевых узлах, которые будут первым, вторым, третьим и т.д. MDS (все MDS на всех серверах можно развёртывать одновременно):
   * В ресурсе `ceph::mds` установить параметр `$ensure` в `running`
   * Применить модуль ceph одновременно к первому, второму, третьему и последующим MDS.
при этом, по умолчанию, на этот сервер установятся OSD (на HDD и RAM), а также CLI.

1. Для установки OSD и CLI на оставшиеся сервера кластера ceph необходимо применить к ним модуль ceph. Нет необходимости корректировать какие либо параметры. Все OSD и CLI на всех оставшихся серверах можно развёртывать одновременно.

1. После развёртывания всех сервисов кластера есть возможность приступить к созданию pools, необходимо на последнем сервере, на котором развёртывали OSD и CLI:
   * В ресурсах `ceph::pool::hdd` и `ceph::pool::ram` установить параметр `$ensure` в `running`
   * Подумать, менять ли значение у параметра `$max_mds_fss` с заданного по умолчанию. При увеличении параметра `$max_mds_fss` следует дополнительно развернуть соответствующее количество MDS. Рекомендации по заполнению параметра `$max_mds_fss` даны в манифесте, в котором объявлен параметр `$max_mds_fss` (`pool.pp`).
   * Заполнить параметры `pg_num_ec и pg_num_replicated` для всех FS. Сейчас у нас две FS на hdd и на ram дисках, соответственно необходимо рассчитать и заполнить четыре значения `PG`.
Параметры `pg_num_ec и pg_num_replicated` нужно заполнить с учётом рекомендаций ниже. Параметры `pg_num_ec и pg_num_replicated` после их установки, можно только увеличить, но это создаст серьёзную нагрузку на кластер ceph.
Помощь в подборе параметров `pg_num_ec и pg_num_replicated` могут оказать следующие источники:  
Устарела ссылка https://docs.ceph.com/docs/mimic/rados/operations/pools  
Устарела ссылка https://docs.ceph.com/docs/mimic/rados/operations/placement-groups/#set-the-number-of-placement-groups  
https://old.ceph.com/pgcalc/  
Параметры `pg_num_ec и pg_num_replicated` важны для кластера ceph и их нужно выбирать очень обдуманно.
В первом приближении, оценивая количество `PG` на кластере, можно ориентироваться на следующую формулу, применять её в работе не рекомендуется:
https://docs.ceph.com/en/nautilus/rados/operations/placement-groups/#choosing-number-of-placement-groups

      $`PGs = \frac{OSDs \times 100}{pool \; size}`$

      Где `PGs` - одно из четырёх рассчитываемых `PG`, `OSDs` - количество OSD в кластере, для которых вычисляется `PG` (для RAM pool, это общее количество RAM дисков на всех серверах, для HDD pool это общее количество HDD дисков на всех серверах), `pool size` - число реплик pool для которого рассчитывается `PG`. Для `replicated pools` у нас `pool size`=`3`. Для `erasure coded pools` `pool size`=`$k` + `$m` (`pool size`=5+2=7) . Полученное значение `PGs` нужно округлить до ближайшей степени 2, вверх или вниз, получится, например: 32, 64, 128, 256, 512, 1024, 2048 и т.д.
   * Применить один раз на последнем сервере, на котором развёртывали OSD и CLI модуль ceph для создания pools.
Для создания pools нельзя применять модуль ceph на сервере имеющем роль MON или MDS.

1. После успешного создания pools есть возможность приступить к монтированию директорий:
   * В КСУ у ресурса `ceph::mount` установить параметры `$ensure_hdd` и `$ensure_ram` в `running`. Так как значение у параметров `$ensure_hdd` и `$ensure_ram` меняем в КСУ, то оно применится для всех серверов кластера ceph, что нам и нужно.
   * В КСУ у ресурса `ceph` заполнить параметр `all_mon_name_data` именами интерфейса передачи данных (Infiniband) всех MONs. Обычно это три - пять серверов. Имена необходимо заключить в кадратные скобки и внутри скобок каждое имя в одинарные кавычки, например: ['cephmon07ib', 'cephmon06ib', 'cephmon05ib'].
   * Применить модуль ceph к серверам, где есть OSD. То есть, ко всем серверам.

Running the tests
-----------------

Проверить работу кластера ceph записав файл в смонтированные директории ceph на одном сервере и проверив читаемость и удаление файла в смонтированных директориях ceph на других серверах.

```{.bash}
date > testfiledate.txt
scp testfiledate.txt user@host1:/var/cephfs/hdd/
scp testfiledate.txt user@host2:/var/cephfs/ram/
ssh user@host3 cat /var/cephfs/hdd/testfiledate.txt
ssh user@host4 cat /var/cephfs/hdd/testfiledate.txt
ssh user@host5 rm -f /var/cephfs/hdd/testfiledate.txt
ssh user@host6 rm -f /var/cephfs/hdd/testfiledate.txt
rm -f testfiledate.txt
```

Где `testfiledate.txt` имя тестового файла, `user` - имя пользователя, `hostX` - имена серверов ceph, `/var/cephfs/hdd/` и `/var/cephfs/ram/` - имена директорий, в которые смонтированы соответствующие pools ceph.
Если при выполнении этих команд не будет ошибок и команды `cat` отобразят содержимое файла `testfiledate.txt`, то можно считать простую проверку директорий ceph на чтение и запись пройденной.

Проверить скорости записи в смонтированные директории ceph

```{.bash}
dd if=/dev/zero of=/var/cephfs/hdd/dd.10G bs=1M count=10000
dd if=/dev/zero of=/var/cephfs/ram/dd.1G bs=1M count=1000
```

Проверить скорости чтения файлов в смонтированных директориях ceph

```{.bash}
dd if=/var/cephfs/ram/dd.1G of=/dev/null
dd if=/var/cephfs/hdd/dd.10G of=/dev/null
```
